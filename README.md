# ü§ñüß† Analysis of RL-Algorithms on the [Sokoban Problem](https://github.com/mpSchrader/gym-sokoban)

In this project, I have analyzed Reinforcement Learning solutions to solve a simplified version of the [Sokoban environment](https://github.com/mpSchrader/gym-sokoban). The notebook provides a comprehensive guide from installation to result discussion.


### Key Points
- Demonstrating the use of [Monte Carlo Off-Policy Control](http://incompleteideas.net/book/ebook/node56.html) and [Deep Q-Network](https://arxiv.org/abs/1312.5602) in order to solve the [Sokoban environment](https://github.com/mpSchrader/gym-sokoban)
- Visualizing the training progress, evaluating the agent's performance, analyzing the learning dynamics and interpreting the results

[‚ñ∂ View the analysis here.](https://github.com/schefflaa/Sokoban-RL/blob/main/main.ipynb)
</br>
</br>

‚ùó Note that to properly display images and use intra-document links downloading the repo is necessary.


| Run 1 | Run 2 | Run 3 | Run 4 | Run 5 | Run 6 |
| :---: | :---: | :---: | :---: | :---: | :---: |
| ![Run 1](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_1/video.gif?raw=true) | ![Run 2](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_2/video.gif?raw=true) | ![Run 3](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_3/video.gif?raw=true) | ![Run 4](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_4/video.gif?raw=true) | ![Run 5](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_5/video.gif?raw=true)| ![Run 6](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_6/video.gif?raw=true) |
| **Run 7** | **Run 8** | **Run 9** | **Run 10** | **Run 11** | **Run 12** |
| ![Run 7](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_7/video.gif?raw=true) | ![Run 8](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_8/video.gif?raw=true) | ![Run 9](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_9/video.gif?raw=true) | ![Run 10](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_10/video.gif?raw=true) | ![Run 11](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_11/video.gif?raw=true) | ![Run 12](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_12/video.gif?raw=true) |
| **Run 13** | **Run 14** | **Run 15** | **Run 16** | **Run 17** | **Run 18** |
| ![Run 13](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_13/video.gif?raw=true) | ![Run 14](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_14/video.gif?raw=true) | ![Run 15](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_15/video.gif?raw=true) | ![Run 16](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_16/video.gif?raw=true) | ![Run 17](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_17/video.gif?raw=true) | ![Run 18](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_18/video.gif?raw=true) |
| **Run 19** | **Run 20** | **Run 21** | **Run 22** | **Run 23** | **Run 24** |
| ![Run 19](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_19/video.gif?raw=true) | ![Run 20](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_20/video.gif?raw=true) | ![Run 21](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_21/video.gif?raw=true) | ![Run 22](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_22/video.gif?raw=true) | ![Run 23](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_23/video.gif?raw=true) | ![Run 24](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_24/video.gif?raw=true) |
| **Run 25** | **Run 26** | **Run 27** | **Run 28** | **Run 29** | **Run 30** |
| ![Run 25](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_25/video.gif?raw=true) | ![Run 26](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_26/video.gif?raw=true) | ![Run 27](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_27/video.gif?raw=true) | ![Run 28](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_28/video.gif?raw=true) | ![Run 29](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_29/video.gif?raw=true) | ![Run 30](./data/logs/DQN_experiments/small_30_avg_5k_eps/DQN_30/video.gif?raw=true) |


